{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "72\n",
      "\n",
      "-1\n",
      "85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import json\n",
    "import random\n",
    "import random\n",
    "\n",
    "documents = []\n",
    "# Open the JSONL file for reading\n",
    "with open('train.jsonl', 'r') as jsonl_file:\n",
    "    # Iterate over the lines in the file\n",
    "    for line in jsonl_file:\n",
    "        # Parse each line as a JSON object\n",
    "        data = json.loads(line)\n",
    "        text = data['text']\n",
    "        starting_point = 0\n",
    "        entities = []\n",
    "        for entry in data['entities']:\n",
    "            substring = entry['surfaceform']\n",
    "            \n",
    "            start_index = text.find(substring)\n",
    "            # if (start_index < starting_point) & (start_index != -1):\n",
    "            #     temp_start = text[starting_point:].find(substring)\n",
    "            #     print(temp_start)\n",
    "            #     print(start_index)\n",
    "            #     end_index = temp_start + len(substring)\n",
    "            #     print(text[temp_start+starting_point:end_index])\n",
    "            \n",
    "            if start_index == -1:\n",
    "                continue\n",
    "            end_index = start_index + len(substring)\n",
    "            starting_point = end_index\n",
    "            \n",
    "            if start_index != entry['boundaries'][0]:\n",
    "                entry['boundaries'] = [start_index, end_index]\n",
    "            \n",
    "            entities.append(entry)\n",
    "\n",
    "        data['entities'] = entities\n",
    "with open(\"output_new.jsonl\", \"a\") as f:\n",
    "    json.dump(data, f)\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_relations(file_name:str):\n",
    "    relations_my = []\n",
    "    # Open the JSONL file for reading\n",
    "    with open('train.jsonl', 'r') as jsonl_file:\n",
    "        # Iterate over the lines in the file\n",
    "        for line in jsonl_file:\n",
    "            # Parse each line as a JSON object\n",
    "            # print(line)\n",
    "            data = json.loads(line)\n",
    "\n",
    "            for entry in data['triples']:\n",
    "                relations_my.append(entry['predicate']['surfaceform'])\n",
    "\n",
    "    return relations_my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import extract_data as ex\n",
    "\n",
    "relations_counter = Counter(ex.count_relations('train.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "relations_counter_df = pd.DataFrame.from_dict(relations_counter, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df = pd.read_csv('./relations_count.tsv', header = None, sep='\\t')\n",
    "relations = list(relations_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_counter_df = relations_counter_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_counter_df.columns = ['word', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df.columns = ['word', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df_new = pd.concat([relations_counter_df, relations_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df_new.to_csv('relations_count.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import random\n",
    "\n",
    "documents = []\n",
    "relations_my = []\n",
    "id_entity = {}\n",
    "entities = []\n",
    "triplets = []\n",
    "# Open the JSONL file for reading\n",
    "with open('chev_data2.json', 'r') as jsonl_file:\n",
    "    # Iterate over the lines in the file\n",
    "    data = json.load(jsonl_file)\n",
    "    # print(data['annotations'])\n",
    "    for item in data:\n",
    "        document = {}\n",
    "        print(item['id'])\n",
    "        doc_id = random.randint(1, 1000000)\n",
    "        document['doc_id'] = doc_id\n",
    "        document['uri'] = item['id']\n",
    "        text = item['data']['text']\n",
    "        document['text'] = text\n",
    "        title = ' '.join(text.split(' ')[:5])\n",
    "        document['title'] = title\n",
    "\n",
    "        # Parse each line as a JSON object\n",
    "        for entry in item['annotations']:\n",
    "            for res in entry['result']:\n",
    "                try:\n",
    "                    entity = {}\n",
    "                    boundaries = []\n",
    "                    \n",
    "                   \n",
    "                    if res['type'] == 'labels':\n",
    "                        uri = res['id']\n",
    "                        value = res['value']\n",
    "                        surfaceform = value['text']\n",
    "                        boundaries.append(value['start'])\n",
    "                        boundaries.append(value['end'])\n",
    "                        entity['uri'] = uri\n",
    "                        entity['surfaceform'] = surfaceform\n",
    "                        entity['boundaries'] = boundaries\n",
    "                        entity['annotator'] = 'Me'\n",
    "\n",
    "                        id_entity[uri] = entity\n",
    "                        entities.append(entity)\n",
    "\n",
    "                    elif res['type'] == 'relation':\n",
    "                        triplet = {}\n",
    "                        from_id = res['from_id']\n",
    "                        to_id = res['to_id']\n",
    "                        surfaceform = res['labels'][0]\n",
    "                        predicate = {}\n",
    "                        uri_pred_len = min(len(from_id), len(to_id))\n",
    "                        sample_list = random.sample(from_id+to_id, k=uri_pred_len)\n",
    "                        # Join the list of characters into a single string\n",
    "                        sample_string = ''.join(sample_list)\n",
    "\n",
    "                        predicate['uri'] = sample_string\n",
    "                        predicate['annotator'] = 'Me'\n",
    "                        predicate['surfaceform'] = surfaceform\n",
    "                        predicate['boundaries'] = None\n",
    "\n",
    "                        relations_my.append(surfaceform)\n",
    "\n",
    "                        if res['direction'] == 'right':\n",
    "                            triplet['subject'] = id_entity[from_id]\n",
    "                            triplet['object'] = id_entity[to_id]\n",
    "                            triplet['predicate'] = predicate\n",
    "\n",
    "                            triplets.append(triplet)\n",
    "                        else:\n",
    "                            print(res)\n",
    "                except Exception as e:\n",
    "                    print(res)\n",
    "                    print(e)\n",
    "                    break\n",
    "        \n",
    "        document['entities'] = entities\n",
    "        document['triples'] = triplets\n",
    "        documents.append(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'protect': 2,\n",
       "         'support': 8,\n",
       "         'reduce': 7,\n",
       "         'request': 2,\n",
       "         'aim': 5,\n",
       "         'stop': 3,\n",
       "         'of': 3,\n",
       "         'protect against': 3,\n",
       "         'is': 1,\n",
       "         'from': 1,\n",
       "         'is required': 4,\n",
       "         'identify': 2})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(relations_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entites_spacy(file_name):\n",
    "    documents = []\n",
    "    # Open the JSONL file for reading\n",
    "    with open(file_name, 'r') as jsonl_file:\n",
    "        # Iterate over the lines in the file\n",
    "        data = json.load(jsonl_file)\n",
    "        for item in data:\n",
    "            text = item['data']['text']\n",
    "\n",
    "            entities = []\n",
    "            # Parse each line as a JSON object\n",
    "            for entry in item['annotations']:\n",
    "                for res in entry['result']:\n",
    "                    try:\n",
    "                        boundaries = []\n",
    "                    \n",
    "                        if res['type'] == 'labels':\n",
    "                            value = res['value']\n",
    "                            from_name = res['from_name']\n",
    "                            ner = from_name.split('-')[1]\n",
    "                            if ner == 'General':\n",
    "                                continue\n",
    "                            boundaries.append(value['start'])\n",
    "                            boundaries.append(value['end'])\n",
    "                            entities.append((boundaries[0], boundaries[1], ner))\n",
    "                    except Exception as e:\n",
    "                        print(res)\n",
    "                        print(e)\n",
    "                        break\n",
    "            \n",
    "            documents.append((text, {'entities': entities}))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "from extract_data import get_entites_spacy\n",
    "documents = []\n",
    "documents += get_entites_spacy('chev_data.json')\n",
    "documents += get_entites_spacy('chev_data2.json')\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223\n",
      "{'from_id': 'gm2nXX5EIp', 'to_id': '_vpFR1OHM7', 'type': 'relation', 'direction': 'right', 'labels': []}\n",
      "list index out of range\n",
      "230\n",
      "{'from_id': 'fSjdyI5ORI', 'to_id': 'gzXgGzbIg7', 'type': 'relation', 'direction': 'right', 'labels': []}\n",
      "list index out of range\n",
      "231\n",
      "1\n",
      "14\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import extract_data as ex\n",
    "documents = []\n",
    "documents += ex.get_relations_from_label_studio_data('./chev_data.json')\n",
    "documents += ex.get_relations_from_label_studio_data('./chev_data2.json')\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# open file for writing\n",
    "with open(\"output.jsonl\", \"a\") as f:\n",
    "    # write each object as a line in the file\n",
    "    for obj in documents:\n",
    "        json.dump(obj, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DATA = [\n",
    "#     (\"Apple is looking at buying U.K. startup for $1 billion\", {\"entities\": [(0, 5, \"ORG\"), (27, 31, \"GPE\"), (44, 52, \"MONEY\")]}),\n",
    "#     (\"Google acquires Kaggle for undisclosed amount\", {\"entities\": [(0, 6, \"ORG\"), (17, 23, \"PRODUCT\"), (31, 38, \"MONEY\")]}),\n",
    "#     # Add more examples of your new entity classes here\n",
    "# ]\n",
    "\n",
    "sentences = [\"Apple is looking at buying U.K. startup for $1 billion\", \"Google acquires Kaggle for undisclosed amount\"] # Add more examples of your new entity classes here\n",
    "labels = [[0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]] # Convert entity labels to integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['country', 'located in the administrative territorial entity', 'contains administrative territorial entity', 'date of birth', 'sport', 'instance of', 'point in time', 'date of death', 'part of', 'shares border with', 'publication date', 'place of birth', 'inception', 'member of sports team', 'genre', 'performer', 'has part', 'cast member', 'country of citizenship', 'participant in', 'parent taxon', 'subclass of', 'location', 'member of political party', 'director', 'author', 'educated at', 'capital', 'league', 'headquarters location', 'record label', 'position held', 'country of origin', 'spouse', 'member of', 'conflict', 'followed by', 'sibling', 'located in or next to body of water', 'owned by', 'father', 'notable work', 'occupation', 'follows', 'publisher', 'part of the series', 'mouth of the watercourse', 'award received', 'child', 'mountain range', 'operator', 'position played on team / speciality', 'winner', 'sports season of league or competition', 'place of death', 'manufacturer', 'named after', 'composer', 'participant', 'applies to jurisdiction', 'creator', 'parent organization', 'founded by', 'start time', 'tributary', 'employer', 'subsidiary', 'present in work', 'developer', 'field of work', 'work period (start)', 'location of formation', 'platform', 'original broadcaster', 'religion', 'located on terrain feature', 'organizer', 'product or material produced', 'capital of', 'diplomatic relation', 'continent', 'end time', 'connecting line', 'facet of', 'distributed by', 'owner of', 'replaced by', 'participating team', 'home venue', 'screenwriter', 'replaces', 'licensed to broadcast to', 'producer', 'sports discipline competed in', 'encodes', 'production company', 'constellation', 'main subject', 'use', 'uses', 'twinned administrative body', 'population', 'military branch', 'family', 'mother', 'ethnic group', 'characters', 'place served by transport hub', 'occupant', 'architect', 'heritage designation', 'opposite of', 'conferred by', 'legislative body', 'instrument', 'architectural style', 'date of official opening', 'crosses', 'dissolved, abolished or demolished date', 'based on', 'language used', 'endemic to', 'connects with', 'season of club or team', 'language of work or name', 'industry', 'religious order', 'discoverer or inventor', 'political ideology', 'area', 'military rank', 'affiliation', 'nominated for', 'school district', 'has parts of the class', 'number of participants', 'elevation above sea level', 'successful candidate', 'time period', 'has effect', 'different from', 'movement', 'officeholder', 'material used', 'court', 'office contested', 'competition class', 'narrative location', 'chairperson', 'work location', 'operating system', 'studied by', 'candidacy in election', 'season', 'influenced by', 'basin country', 'contains settlement', 'subject has role', 'taxonomic type', 'significant event', 'presenter', 'residence', 'located in protected area', 'has cause', 'designed by', 'diocese', 'maintained by', 'appointed by', 'number of episodes', 'depicts', 'place of publication', 'relative', 'practiced by', 'inflows', 'collection', 'student of', 'terminus', 'radio format', 'event distance', 'operating area', 'legislated by', 'lyrics by', 'studies', 'office held by head of government', 'animal breed', 'field of this occupation', 'editor', 'represents', 'voice type', 'site of astronomical discovery', 'said to be the same as', 'highest point', 'has works in the collection', 'historic county', 'authority', 'programming language', 'airline hub', 'stock exchange', 'date of first performance', 'length', 'located on street', 'item operated', 'head of state', 'surface played on', 'carries', 'victory', 'ranking', 'medical condition treated', 'head of government', 'used by', 'librettist', 'start point', 'place of burial', 'is a list of', 'drafted by', 'writing system', 'candidate', 'after a work by', 'derivative work', 'indigenous to', 'office held by head of the organization', 'service entry', 'student', 'lake outflow', 'Gram staining', 'published in', 'original language of film or TV show', 'origin of the watercourse', 'drug used for treatment', 'sports league level', 'liable for', 'support 2', 'set emission reduction targets  1', 'asked about 1', 'testified   1', 'aim 8', 'is  2', 'reduce  4', 'request 10', 'report  20', 'concern 4', 'fail    2', 'from    8', 'adopt   4', 'is responsible  4', 'is required 2']\n"
     ]
    }
   ],
   "source": [
    "relations_df = pd.read_csv('./relations_count.tsv', header = None, sep='\\t')\n",
    "relations = list(relations_df[0])\n",
    "print(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aim', 'is', 'reduce', 'request', 'report', 'concern', 'fail', 'from', 'adopt', 'is responsible', 'is required', 'country', 'located in the administrative territorial entity', 'contains administrative territorial entity', 'date of birth', 'sport', 'instance of', 'point in time', 'date of death', 'part of', 'shares border with', 'publication date', 'place of birth', 'inception', 'member of sports team', 'genre', 'performer', 'has part', 'cast member', 'country of citizenship', 'participant in', 'parent taxon', 'subclass of', 'location', 'member of political party', 'director', 'author', 'educated at', 'capital', 'league', 'headquarters location', 'record label', 'position held', 'country of origin', 'spouse', 'member of', 'conflict', 'followed by', 'sibling', 'located in or next to body of water', 'owned by', 'father', 'notable work', 'occupation', 'follows', 'publisher', 'part of the series', 'mouth of the watercourse', 'award received', 'child', 'mountain range', 'operator', 'position played on team / speciality', 'winner', 'sports season of league or competition', 'place of death', 'manufacturer', 'named after', 'composer', 'participant', 'applies to jurisdiction', 'creator', 'parent organization', 'founded by', 'start time', 'tributary', 'employer', 'subsidiary', 'present in work', 'developer', 'field of work', 'work period (start)', 'location of formation', 'platform', 'original broadcaster', 'religion', 'located on terrain feature', 'organizer', 'product or material produced', 'capital of', 'diplomatic relation', 'continent', 'end time', 'connecting line', 'facet of', 'distributed by', 'owner of', 'replaced by', 'participating team', 'home venue', 'screenwriter', 'replaces', 'licensed to broadcast to', 'producer', 'sports discipline competed in', 'encodes', 'production company', 'constellation', 'main subject', 'use', 'uses', 'twinned administrative body', 'population', 'military branch', 'family', 'mother', 'ethnic group', 'characters', 'place served by transport hub', 'occupant', 'architect', 'heritage designation', 'opposite of', 'conferred by', 'legislative body', 'instrument', 'architectural style', 'date of official opening', 'crosses', 'dissolved, abolished or demolished date', 'based on', 'language used', 'endemic to', 'connects with', 'season of club or team', 'language of work or name', 'industry', 'religious order', 'discoverer or inventor', 'political ideology', 'area', 'military rank', 'affiliation', 'nominated for', 'school district', 'has parts of the class', 'number of participants', 'elevation above sea level', 'successful candidate', 'time period', 'has effect', 'different from', 'movement', 'officeholder', 'material used', 'court', 'office contested', 'competition class', 'narrative location', 'chairperson', 'work location', 'operating system', 'studied by', 'candidacy in election', 'season', 'influenced by', 'basin country', 'contains settlement', 'subject has role', 'taxonomic type', 'significant event', 'presenter', 'residence', 'located in protected area', 'has cause', 'designed by', 'diocese', 'maintained by', 'appointed by', 'number of episodes', 'depicts', 'place of publication', 'relative', 'practiced by', 'inflows', 'collection', 'student of', 'terminus', 'radio format', 'event distance', 'operating area', 'legislated by', 'lyrics by', 'studies', 'office held by head of government', 'animal breed', 'field of this occupation', 'editor', 'represents', 'voice type', 'site of astronomical discovery', 'said to be the same as', 'highest point', 'has works in the collection', 'historic county', 'authority', 'programming language', 'airline hub', 'stock exchange', 'date of first performance', 'length', 'located on street', 'item operated', 'head of state', 'surface played on', 'carries', 'victory', 'ranking', 'medical condition treated', 'head of government', 'used by', 'librettist', 'start point', 'place of burial', 'is a list of', 'drafted by', 'writing system', 'candidate', 'after a work by', 'derivative work', 'indigenous to', 'office held by head of the organization', 'service entry', 'student', 'lake outflow', 'Gram staining', 'published in', 'original language of film or TV show', 'origin of the watercourse', 'drug used for treatment', 'sports league level']\n",
      "<triplet> Investor groups <subj> companies  <obj> request <subj> financial reporting <obj> request <triplet> companies  <subj> climate-related risks <obj> report <triplet> fossil fuel producers <subj> carbon economy <obj> reduce <triplet> Investors  <subj> companies  <obj> concern <triplet> companies  <subj> financial statements <obj> report\n",
      "<triplet> climate change <subj> profits  <obj> report <subj> assets <obj> report\n",
      "<triplet> companies  <subj> climate change <obj> report <triplet> auditors  <subj> climate change <obj> report\n",
      "<triplet> compliance  <subj> International Accounting Standards Board (IASB) <obj> from <subj> climate-related risks <obj> report\n",
      "<triplet> energy <subj> net zero carbon <obj> aim\n",
      "<triplet> Mark Carney <subj> Investors  <obj> concern <triplet> Investors  <subj> weather events <obj> adopt <subj> climate policies <obj> adopt <triplet> weather events <subj> net zero <obj> aim <triplet> climate policies <subj> net zero <obj> aim <triplet> net zero <subj>  financial impact <obj> is responsible <subj> quantity, quality and comparability of reporting <obj> is required <triplet> quantity, quality and comparability of reporting <subj> net zero <obj> report <subj> companies <obj> from <triplet> reporting <subj> companies <obj> from <triplet> core financial reports <subj> net zero <obj> report <subj> companies <obj> from\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<triplet> Investor groups <subj> companies  <obj> request <subj> financial reporting <obj> request <triplet> companies  <subj> climate-related risks <obj> report <triplet> fossil fuel producers <subj> carbon economy <obj> reduce <subj> stabilising global temperatures <obj> reduce <triplet> Investors  <subj> companies  <obj> concern <triplet> companies  <subj> financial statements <obj> report <subj> climate change <obj> fail <triplet> climate change <subj> profits  <obj> report <subj> assets <obj> report\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "\n",
    "relations_df = pd.read_csv('./relations_count.tsv', header = None, sep='\\t')\n",
    "relations = list(relations_df[0])\n",
    "print(relations)\n",
    "with open('./train.jsonl', encoding=\"utf-8\") as f:\n",
    "    for id_, row in enumerate(f):\n",
    "        if id_ >= 1000:\n",
    "            print(id)\n",
    "            break\n",
    "        article = json.loads(row)\n",
    "        prev_len = 0\n",
    "        # print(article)\n",
    "        if len(article['triples']) == 0:\n",
    "            continue\n",
    "        count = 0\n",
    "        for text_paragraph in article['text'].split('\\n'):\n",
    "            if len(text_paragraph) == 0:\n",
    "                continue\n",
    "            # print(text_paragraph)\n",
    "            sentences = re.split(r'(?<=[.])\\s', text_paragraph)\n",
    "            text = ''\n",
    "            for sentence in sentences:\n",
    "                text += sentence + ' '\n",
    "                if any([entity['boundaries'][0] < len(text) + prev_len < entity['boundaries'][1] for entity in article['entities']]):\n",
    "                    continue\n",
    "                entities = sorted([entity for entity in article['entities'] if prev_len < entity['boundaries'][1] <= len(text)+prev_len], key=lambda tup: tup['boundaries'][0])\n",
    "                decoder_output = '<triplet> '\n",
    "                # print(entities)\n",
    "                for int_ent, entity in enumerate(entities):\n",
    "                    # for triplet in article['triples']:\n",
    "                    #     print(triplet)\n",
    "                    #     print(prev_len < triplet['subject']['boundaries'][1] <=len(text) + prev_len)\n",
    "                    #     print(prev_len < triplet['object']['boundaries'][1] <=len(text) + prev_len)\n",
    "                    #     print(triplet['predicate']['surfaceform'] in relations)\n",
    "                    triplets = sorted([triplet for triplet in article['triples'] \n",
    "                                       if triplet['subject'] == entity and prev_len< triplet['subject']['boundaries'][1]<=len(text) + prev_len and prev_len< triplet['object']['boundaries'][1]<=len(text) + prev_len and triplet['predicate']['surfaceform'] in relations], key=lambda tup: tup['object']['boundaries'][0])\n",
    "                    # print(triplets)\n",
    "                    if len(triplets) == 0:\n",
    "                        continue\n",
    "                    decoder_output += entity['surfaceform'] + ' <subj> '\n",
    "                    for triplet in triplets:\n",
    "                        decoder_output += triplet['object']['surfaceform'] + ' <obj> '  + triplet['predicate']['surfaceform'] + ' <subj> '\n",
    "                    decoder_output = decoder_output[:-len(' <subj> ')]\n",
    "                    decoder_output += ' <triplet> '\n",
    "                decoder_output = decoder_output[:-len(' <triplet> ')]\n",
    "                count += 1\n",
    "                prev_len += len(text)\n",
    "                print(decoder_output)\n",
    "                if len(decoder_output) == 0:\n",
    "                    text = ''\n",
    "                    continue\n",
    "\n",
    "                text = re.sub('([\\[\\].,!?()])', r' \\1 ', text.replace('()', ''))\n",
    "                text = re.sub('\\s{2,}', ' ', text)\n",
    "\n",
    "\n",
    "                text = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
